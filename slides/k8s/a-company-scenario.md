# Using Kubernetes in an Enterprise-like scenario

- ğŸ’ªğŸ¼ Okay. The former training modules explain each subject in-depth, and each feature one at-a-time

- ğŸ¤¯ One of the first complexity any `k8s` admin encounters resides in having to choose and assemble all these moving parts to build a _Production-ready_ `k8s` cluster

- ğŸ¯ This module precisely aims to play a classic corporate scenario and see what we'll have to do to build such a Prod-ready Kubernetes cluster

- As we've done it before, we'll start to build our cluster from scratch and improve it step by step, by adding a feature one after another

---

## The plan

In a company, we have 3 different people, _**âš™ï¸OPS**_, _**ğŸ¸ROCKY**_ and _**ğŸ»CLASSY**_

- _**âš™ï¸OPS**_ is the platform engineer building and configuring Kubernetes clusters

- Both _**ğŸ¸ROCKY**_ and _**ğŸ»CLASSY**_ build Web apps that manage ğŸ’¿ record collections

    - _**ğŸ¸ROCKY**_ builds a Web app managing _rock & pop_ collection
    - whereas _**ğŸ»CLASSY**_ builds a Web app for _classical_ collection

- Each app is stored in its own git repository

- Both _**ğŸ¸ROCKY**_ and _**ğŸ»CLASSY**_ want to code, build package and deploy their applications onto a `Kubernetes` _cluster_ _in an autonomous way_

---

### What our scenario might look likeâ€¦

<pre class="mermaid">
%%{init:
    {
      "theme": "default",
      "gitGraph": {
        "mainBranchName": "ops",
        "mainBranchOrder": 0
      }
    }
}%%
gitGraph
    commit id:"0" tag:"start"
    branch ROCKY order:3
    branch CLASSY order:4

    checkout ops
    commit id:'TEST cluster creation' tag:'T01'
    branch TEST-cluster order:1
    commit id:'FLUX install on TEST' tag:'T02'

    checkout ops
    commit id:'TEST cluster config.' tag:'T03'
    checkout TEST-cluster
    merge ops id:'setup of TEST cluster' tag:'T04'

    checkout ops
    commit id:'FLUX config for ROCKY deployment' tag:'R01'

    checkout TEST-cluster
    merge ops id:'FLUX ready to deploy ROCKY' type: HIGHLIGHT tag:'R02'

    checkout ROCKY
    commit id:'ROCKY' tag:'v1.0'

    checkout TEST-cluster
    merge ROCKY tag:'ROCKY v1.0'
    
    checkout CLASSY
    commit id:'CLASSY' tag:'v1.0'

    checkout CLASSY
    commit id:'CLASSY HELM chart' tag:'C01'

    checkout ops
    commit id:'FLUX config for CLASSY deployment' tag:'C02'
    checkout TEST-cluster
    merge ops id:'FLUX ready to deploy CLASSY' type: HIGHLIGHT tag:'C03'

    checkout TEST-cluster
    merge CLASSY tag:'CLASSY v1.0'
    
    checkout ROCKY
    commit id:'new color' tag:'v1.1'
    checkout TEST-cluster
    merge ROCKY tag:'ROCKY v1.1'

    checkout TEST-cluster
    commit id:'wrong namespace' type: REVERSE

    checkout ops
    commit id:'namespace isolation'
    checkout TEST-cluster
    merge ops type: HIGHLIGHT

    checkout ROCKY
    commit id:'fix namespace' tag:'v1.1.1'
    checkout TEST-cluster
    merge ROCKY tag:'ROCKY v1.1.1'

    checkout ROCKY
    commit id:'add a field' tag:'v1.2'
    checkout TEST-cluster
    merge ROCKY tag:'ROCKY v1.2'

    checkout ops
    commit id:'Kyverno install'
    commit id:'Kyverno rules'
    checkout TEST-cluster
    merge ops type: HIGHLIGHT

    checkout ops
    commit id:'Network policies'
    checkout TEST-cluster
    merge ops type: HIGHLIGHT

    checkout ops
    branch PROD-cluster order:2
    commit id:'FLUX install on PROD'
    commit id:'_**ğŸšœPROD**_ cluster configuration'

    checkout ops
    commit id:'Add OpenEBS'
    checkout TEST-cluster
    merge ops id:'patch dedicated to PROD' type: REVERSE
    checkout PROD-cluster
    merge ops type: HIGHLIGHT
</pre>

---

### Using two Kubernetes clusters
<!-- TODO: choose how to deploy k8s cluster -->

We want to have 2 main `Kubernetes` clusters, one for **testing** and one for **production**

- we should use tools to **industrialise creation** of both clusters

- each cluster has it's **own lifecycle** (the addition or configuration of extra components/features may be done on one cluster and not the other)
- yet, configurations of these clusters must be as centralized as possible (to avoid inconsistency and to limit configuration code expansion and maintainance)

> ğŸ’» we'll use `Flux` to configure and deploy new resources onto the clusters

---

### Multi-tenancy

Both _**ğŸ¸ROCKY**_ and _**ğŸ»CLASSY**_ should use a **dedicated _"tenant"_** on each cluster

- _**ğŸ¸ROCKY**_ should be able to deploy, upgrade and configure its own app in its dedicated **namespace** without anybody else involved

- and the same for _**ğŸ»CLASSY**_

- neither conflict nor collision should be allowed between the 2 apps or the 2 teams.

---

### How to create the _**âš—ï¸TEST**_ cluster

ğŸš§ First we have to create a _**âš—ï¸TEST**_ cluster
- on-premise (Linux machine)
- single node
- no HA
- very easy to deploy / trash / recreate
- for the dev teams to deploy and test their apps
- for the **ops** team to test their new configurations and tools

<!-- FIXME: find some kind of emphatic style -->
> ğŸ’» we'll use `k0s` to deploy it

---

### How to create the _**ğŸšœPROD**_ cluster

ğŸš§ Second, we will create a _**ğŸšœPROD**_ cluster
- in the Cloud for availability and scalability purpose (let's say DigitalOcean)
- 3 nodes
- HA

> ğŸ’» we'll use `clusterAPI` from our _**âš—ï¸TEST**_ cluster to deploy it

---

### 1st development team  _**ğŸ¸ROCKY**_

Our first development team is developping an app to manage **rock & pop** music records.

Here is a what it looks like :

<!-- TODO: include screenshot -->

---

### The code

Here is the codeâ€¦

<!-- TODO: include screenshot -->

---

### How to deploy?

- The app is to be deployed on Kubernetes cluster, in a dedicated `rocky` namespace.  
- So the _**ğŸ¸ROCKY**_ team produces a YAML file to deploy the whole needed Kubernetes resources to run this application.

<!-- TODO: move into the step-by-step part -->

.lab[

- Review the deployment manifest:
  ```bash
  cd apps/rocky/
  cat ./deployment.yaml
  ```

]

Then, the _**ğŸ¸ROCKY**_ team will have to apply its deployment using this command  

```bash
kubectl apply -f ./deployment.yaml
```

---

### Deploying in an autonomous wayâ€¦


To do so, the _**ğŸ¸ROCKY**_ team requires

- âš ï¸ a **network connection** between its workstation (where `kubectl` is executed) and both Kubernetes clusters

- âš ï¸ an enabled **account** to operate onto the Kubernetes clusters

- âš ï¸ an always _up-to-date_ `kubectl` config with **valid credentials**

- âš ï¸ a `kubectl` CLI in a version compatible with both Kubernetes clusters

- ğŸ”’ if this command is executed by a _CI/CD pipeline_, it will need the same requirements

ğŸ’¡ There might be a better way. With GitOps! ğŸ¾

---

# Incoming Flux

ğŸ’¡ We'll use `Flux` so that deployments will be directly executed from inside the Kubernetes clusters.

- The _**âš™ï¸OPS**_ team will proceed to configure GitOps

  - to configure the Kubernetes clusters

  - for the _**ğŸ¸ROCKY**_ team, `Flux` will check the app source code Github repository and deploy every time the right git event is triggered

  - for the _**ğŸ»CLASSY**_ team, `Flux` will check every time a new Helm Chart release is published in the Helm Charts repository storing the app

---

## Configuring Flux for _**ğŸ¸ROCKY**_ team

What the _**âš™ï¸OPS**_ team has to do:

- ğŸ”§ Create a dedicated `rocky` _tenant_ on _**âš—ï¸TEST**_ cluster
- ğŸ”§ Create the `Flux` Github source pointing to the _**rocky**_ app source code repository
- ğŸ”§ Add a `kustomize` patch into the global `Flux` config to include the `Flux` configuration for the _**rocky**_ app

What the _**ğŸ¸ROCKY**_ team has to do:

- ğŸ‘¨â€ğŸ’» Creating the `kustomize` file in the _**ğŸ¸ROCKY**_ app source code repository in Github.

---

## Creating the dedicated `rocky` tenant

- Using the `flux` _CLI_, we create the file configuring the tenant for the _**ğŸ¸ROCKY**_ team
- This is done in the global mutualized `base` configuration for both Kubernetes clusters

.lab[

- Review the deployment manifest:
  ```bash
  
$ mkdir -p ./tenants/base/rocky
$ flux create tenant rocky            \
    --with-namespace=rocky-ns         \
    --cluster-role=rocky-full-access  \
    --export > ./tenants/base/rocky/rbac.yaml  ```

]

---

<!-- Here begins the step-by-step part -->
# T01- _**âš—ï¸TEST**_ cluster creation

On a Linux server, we do install a single node `k0s` cluster.

---

## _**âš—ï¸TEST**_ cluster - How to configure?

We want to configure our _**âš—ï¸TEST**_ cluster in a *-as-code and reusable way
â¡ï¸ `Flux`

---

## Flux - what is it?

- Flux est un outil qui permet de faire du GitOps sur Kubernetes
- Il scrute des sources qui vont servir Ã  injecter des descriptions de ressources dans Kubernetes

---

### Flux CLI

- une CLI permet :
  1. de crÃ©er les fichiers `YAML` pour dÃ©ployer les ressources `Kubernetes` que l'on souhaite
     - y compris les propres composants `Flux`
  1. d'interagir avec le dÃ©pÃ´t `git` qui va servir de configuration `Flux`
  1. d'interroger l'Ã©tat de Flux sur le _cluster_
     - logs des _operators_
     - _CRD_

---

### Flux architecture

![Flux architecture](images/flux_schema.jpg)

---

### Flux components

- `source controller` pour scruter les sources de configuration depuis des dÃ©pÃ´ts `git`
- `helm controller` pour dÃ©tecter de nouvelles _releases_ depuis des dÃ©pÃ´ts de _charts_ `Helm`
- des _CRD_, qui servent de machine Ã  Ã©tat pour stocker la configuration dans le _cluster_

---

### Flux -- for more info

Reference to the `Flux` chapter in High Five M3 module

---

### Flux relies on Kustomize

- `kustomize controller` qui passe la configuration trouvÃ©e Ã  `Kustomize`
    1. `Kustomize` consolide la configuration trouvÃ©e
    2. et hydrate les sections template prÃ©sentes dans la configuration

---

### Kustomize -- for more info

Reference to the `Kustomize` chapter in High Five M3 module

---

## T02- _**âš—ï¸TEST**_ cluster - Installing Flux

Ã€ partir de lÃ , toute la configuration du _cluster_ Kubernetes peut se faire exclusivement en manipulant des dÃ©pÃ´ts `git`.

On est dans le respect du _pattern_ roi dans `Kubernetes` : **la convergence vers un Ã©tat cible dÃ©crit**

---

### Flux CLI 1

1. L'Ã©quipe **ops** rÃ©cupÃ¨re la _CLI_ `Flux` sur son poste de travail
1. La _CLI_ va s'appuyer sur la configuration `kubectl` pour interagir avec le _cluster_
   - connectivitÃ© rÃ©seau
   - droits _RBAC_

----

### Flux install - Checking prerequisites

1. On s'assure que
   - la CLI se connecte bien
   - que les versions de `Flux` et de `Kubernetes` sont OK

```bash
$ flux check ---pre
â–º checking prerequisites
âœ” Kubernetes 1.21.9-gke.1002 >=1.20.6-0
âœ” prerequisites checks passed
```

----

### Git repo to host Flux configuration

- La CLI va crÃ©er un dÃ©pÃ´t `fleet-infra` dans notre organisation `Github` : `one-kubernetes`
- Elle a besoin d'un _token_ `Github` capable de _CRUD_ sur les dÃ©pÃ´ts.

----

### Github - Generate a personnal access token

![Generate a Github personnal access token](images/github_add_token.png)

----

### Github configuration by Flux

- `Flux` peut aussi indiquer les Ã©quipes qui auront le droit de modifier cette configuration.
- Il faut que ces Ã©quipes fassent dÃ©jÃ  partie de l'organisation.

![Teams in Github](images/github_teams.jpg)

----

### Disclaimer

- âš ï¸ Ici on vous montre pour l'exemple, mais dans le reste du _workshop_ (comme dans la vraie vie) les diffÃ©rentes Ã©quipes n'ont pas besoin d'accÃ©der Ã  ce dÃ©pÃ´t.

- C'est tout l'avantage des sources de configuration multiples.

----

### T03- Creating dedicated `Github` repo to host Flux config

```bash [1-3|5-10]
$ export GITHUB_TOKEN="<insert your Github personal token here>"
$ export GITHUB_USER="one-kubernetes"
$ export GITHUB_REPO="fleet-infra"

$ flux bootstrap github         \
    --owner=${GITHUB_USER}      \
    --repository=${GITHUB_REPO} \
    --team=rocky                 \
    --team=classy                 \
    --path=clusters/mycluster

â–º connecting to github.com
âœ” repository "https://github.com/one-kubernetes/fleet-infra" created
â–º reconciling repository permissions
âœ” granted "maintain" permissions to "rocky"
âœ” granted "maintain" permissions to "classy"
âœ” reconciled repository permissions
â–º cloning branch "main" from Git repository "https://github.com/one-kubernetes/fleet-infra.git"
âœ” cloned repository
â–º generating component manifests
âœ” generated component manifests
âœ” committed sync manifests to "main" ("b4906bb66eca7296ba28f0c83808d6de143f930f")
â–º pushing component manifests to "https://github.com/one-kubernetes/fleet-infra.git"
âœ” installed components
âœ” reconciled components
â–º determining if source secret "flux-system/flux-system" exists
â–º generating source secret
âœ” public key: ecdsa-sha2-nistp384 AAAAE2VjZHNhLXNoYTItbmlzdHAzODQAAAAIbmlzdHAzODQAAABhBPWKYVtQ6aCxQMMGRt+HqYD/JRC4sSQtdacQNMs+qhoppVH2+kNMnWIEl8LpJO1szfM2/d+gu3O1bg4T+WkEHgmepO1AYDpO8zmR3uMgeRg7IPeZY3E2BgVaKvfdRuDs6g==
âœ” configured deploy key "flux-system-main-flux-system-./clusters/mycluster" for "https://github.com/one-kubernetes/fleet-infra"
â–º applying source secret "flux-system/flux-system"
âœ” reconciled source secret
â–º generating sync manifests
âœ” generated sync manifests
âœ” committed sync manifests to "main" ("d076136fc7ffaac5f215ec706f56aac5af3de42c")
â–º pushing sync manifests to "https://github.com/one-kubernetes/fleet-infra.git"
â–º applying sync manifests
âœ” reconciled sync configuration
â— waiting for Kustomization "flux-system/flux-system" to be reconciled
âœ” Kustomization reconciled successfully
â–º confirming components are healthy
âœ” helm-controller: deployment ready
âœ” kustomize-controller: deployment ready
âœ” notification-controller: deployment ready
âœ” source-controller: deployment ready
âœ” all components are healthy
```

----

### Flux config files

![Flux config files](images/flux_config_files.jpg)

----

### ğŸ“„ gotk-sync.yaml

```yaml [2-14|15-27]
# This manifest was generated by flux. DO NOT EDIT.
---
apiVersion: source.toolkit.fluxcd.io/v1beta1
kind: GitRepository
metadata:
  name: flux-system
  namespace: flux-system
spec:
  interval: 1m0s
  ref:
    branch: main
  secretRef:
    name: flux-system
  url: ssh://git@github.com/one-kubernetes/fleet-infra
---
apiVersion: kustomize.toolkit.fluxcd.io/v1beta2
kind: Kustomization
metadata:
  name: flux-system
  namespace: flux-system
spec:
  interval: 10m0s
  path: ./clusters/mycluster
  prune: true
  sourceRef:
    kind: GitRepository
    name: flux-system
```

----

### ğŸ“„ kustomization.yaml

```yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
- gotk-components.yaml
- gotk-sync.yaml
```

----

### Cloning the git repo locally

```bash
git clone https://github.com/${GITHUB_USER}/${GITHUB_REPO}
```

---

## _**âš—ï¸TEST**_ cluster - creating the Flux config

- **ops** va avoir Ã  gÃ©rer 2 clusters : _**âš—ï¸TEST**_ et _**ğŸšœPROD**_
- Grace Ã  _Kustomize_, elle va
  1. crÃ©er une config. de base
  2. qui sera surchargÃ©e par une config. spÃ©cifique au _tenant_
- ğŸ’¡Ã‡a paraÃ®t compliquÃ©, mais pas d'inquiÃ©tude : la _CLI_ `Flux` s'occupe de l'essentiel

----

![Multi-tenants clusters](images/cluster_multi_tenants.jpg)

----

### The command

```bash [1|2-8]
$ cd ./fleet-infra
$ flux create kustomization tenants    \
    --namespace=flux-system            \
    --source=GitRepository/flux-system \
    --path ./tenants/test           \
    --prune                            \
    --interval=3m                      \
    --export >> clusters/mycluster/tenants.yaml
```

----

### ğŸ“„ tenants.yaml

```yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1beta2
kind: Kustomization
metadata:
  name: tenants
  namespace: flux-system
spec:
  interval: 5m0s
  path: ./tenants/test
  prune: true
  sourceRef:
    kind: GitRepository
    name: flux-system
```

----

### T04- Don't forget to commit!

> âš ï¸ N'oubliez pas de `git commit` et `git push` vers Github : c'est la source qui va Ãªtre scrutÃ©e par `Flux`.

---

# R01- Configuring _**ğŸ¸ROCKY**_ deployment with Flux

----

## Creating the _tenant_ dedicated to _**ğŸ¸ROCKY**_ team on _**âš—ï¸TEST**_ cluster

```bash
$ mkdir -p ./tenants/base/rocky
$ flux create tenant rocky            \
    --with-namespace=rocky-ns         \
    --cluster-role=rocky-full-access  \
    --export > ./tenants/base/rocky/rbac.yaml
```

----

### ğŸ“„ ./tenants/base/rocky/rbac.yaml

```yaml [1-7|9-16|18-36]
---
apiVersion: v1
kind: Namespace
metadata:
  labels:
    toolkit.fluxcd.io/tenant: rocky
  name: rocky-ns

---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    toolkit.fluxcd.io/tenant: rocky
  name: rocky
  namespace: rocky-ns

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    toolkit.fluxcd.io/tenant: rocky
  name: rocky-reconciler
  namespace: rocky-ns
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: rocky-full-access
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: gotk:rocky-ns:reconciler
- kind: ServiceAccount
  name: rocky
  namespace: rocky-ns
```

----

## _namespace_ isolation for _**ğŸ¸ROCKY**_

```bash
$ cat << EOF | tee ./tenants/base/rocky/cluster-role-rocky.yaml
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  namespace: rocky-ns
  name: rocky-full-access
rules:
- apiGroups: ["", "extensions", "apps"]
  resources: ["deployments", "replicasets", "pods", "services", "ingresses"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"] # You can also use ["*"]
EOF
```

----

## Creating `Github` source in Flux for _**ğŸ¸ROCKY**_ git repository

```bash [1-5|6-10|11-13]
$ flux create source git dev1-aspicot                         \
    --namespace=rocky-ns                                       \
    --url=https://github.com/one-kubernetes/dev1-aspicot-app/ \
    --branch=main                                             \
    --export > ./tenants/base/rocky/sync.yaml
$ flux create kustomization rocky        \
    --namespace=rocky-ns                 \
    --service-account=rocky              \
    --source=GitRepository/dev1-aspicot \
    --path="./" --export >> ./tenants/base/rocky/sync.yaml
$ cd ./tenants/base/rocky/
$ kustomize create --autodetect
$ cd -
```

----

### ğŸ“„ ./tenants/base/rocky/sync.yaml

```yaml [1-11|13-26]
---
apiVersion: source.toolkit.fluxcd.io/v1beta1
kind: GitRepository
metadata:
  name: dev1-aspicot
  namespace: rocky-ns
spec:
  interval: 1m0s
  ref:
    branch: main
  url: https://github.com/one-kubernetes/dev1-aspicot-app/

---
apiVersion: kustomize.toolkit.fluxcd.io/v1beta2
kind: Kustomization
metadata:
  name: rocky
  namespace: rocky-ns
spec:
  interval: 1m0s
  path: ./
  prune: false
  serviceAccountName: rocky
  sourceRef:
    kind: GitRepository
    name: dev1-aspicot
```

----

### ğŸ“„ ./tenants/base/rocky/kustomization.yaml

```yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
- cluster-role-rocky.yaml
- rbac.yaml
- sync.yaml

```

----

### Synchronizing Flux config with its Github repo

AprÃ¨s git commit && git push, on obtient cette arborescence.

![rocky config files](images/dev1_config_files.jpg)

----

## R02- Creating the kustomization in the ROCKY source code repository



- `Flux` scrute le dÃ©pÃ´t de _**ğŸ¸ROCKY**_, mais il s'attend Ã  y trouver un fichier `kustomization.yaml`
- _**ğŸ¸ROCKY**_ doit donc y crÃ©er ce fichier

```bash
kustomize create --autodetect
```

----

## Adding a kustomize patch for _**âš—ï¸TEST**_ cluster deployment


```bash [1|2-10|11-20]
$ mkdir -p ./tenants/test/rocky
$ cat << EOF | tee ./tenants/test/rocky/rocky-patch.yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1beta1
kind: Kustomization
metadata:
  name: rocky
  namespace: rocky-ns
spec:
  path: ./
EOF
cat << EOF | tee ./tenants/test/rocky/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ../../base/rocky
patches:
  - path: rocky-patch.yaml
    target:
      kind: Kustomization
EOF
```

----

## The whole configuration waterfall

![Flux configuration waterfall](images/flux_config_dependencies.jpg)

----

## Rocky app is deployed on _**âš—ï¸TEST**_ cluster

![rocky Web site](images/dev1_website001.png)

----

## âš ï¸ Limitations

- Pour chaque nouveau dÃ©pÃ´t applicatif, **ops** doit ajouter une source `Flux`

- _**ğŸ¸ROCKY**_ est celui qui produit le `deployment.yaml`
    - et donc, **ops** a peu de latitude pour configurer des comportements diffÃ©rents entre _**âš—ï¸TEST**_ et _**ğŸšœPROD**_

- Ã§a veut dire aussi que _**ğŸ¸ROCKY**_ team est seule Ã  dÃ©cider de son architecture technique (service, base de donnÃ©es, etc.) ce qui peut Ãªtre utile en TEST, moins rÃ©aliste en _**ğŸšœPROD**_.

---

# Configuring _**ğŸ»CLASSY**_ deployment with Flux and Helm Chart

For _**ğŸ»CLASSY**_ team, we adopt another strategy that is to deploy with a Helm charts

---

## Why using Helm charts?

- Because dev can just be working on their app source code
- And the final packaging (including technical architecture decisions) might be made available by another team
- And finally, app and stack configuration might be done by the deploying team (meaning either _**ğŸ»CLASSY**_ or ops depending on the ENV)

---

### C01- Creating the Helm chart for _**ğŸ»CLASSY**_ app deployment

ğŸš§ TBD. see: https://github.com/one-kubernetes/classy-helm-charts/tree/main/charts/dev2-carapuce

---

### Publishing Helm chart in an Chart repository

ğŸš§ TBD. see: https://github.com/one-kubernetes/classy-helm-charts/tree/main/charts/dev2-carapuce

---

### Helm - for more info

Reference to the `Flux` chapter in High Five M3 module

---

## C02- Creating the _tenant_ dedicated to _**ğŸ»CLASSY**_ team on _**âš—ï¸TEST**_ cluster

CrÃ©er le tenant dÃ©diÃ© Ã  _**ğŸ»CLASSY**_ se fait de la mÃªme maniÃ¨re que pour _**ğŸ¸ROCKY**_

1. crÃ©ation de l'arborescence de configuration du _tenant_
2. crÃ©ation du _namespace_
3. isolation du _namespace_ (ServiceAccount, RoleBinding, Role)

----

## Creating the `Helm` source in Flux for _**ğŸ»CLASSY**_ Helm chart

âš ï¸ LÃ  par contre, les choses changent !

On ne se source plus depuis un dÃ©pÃ´t `git` mais depuis un dÃ©pÃ´t de _charts_ `Helm`

```bash [1-4]
$ flux create source helm charts                            \
    --url=https://one-kubernetes.github.io/classy-helm-charts \
    --interval=3m                                           \
    --export > ./tenants/base/classy/sync.yaml
```

----

## Creating the `HelmRelease` in Flux

```bash [1-7|9]
$ flux create helmrelease dev2-carapuce        \
    --namespace=classy-ns                        \
    --service-account=classy                     \
    --source=HelmRepository/charts.flux-system \
    --chart=dev2-carapuce-helm                 \
    --chart-version="0.1.0"                    \
    --export >> ./tenants/base/classy/sync.yaml

$ cd ./tenants/base/classy/ && kustomize create --autodetect
```

----

### ğŸ“„ ./tenants/base/classy/sync.yaml

```yaml [1-9|11-16|17-27]
---
apiVersion: source.toolkit.fluxcd.io/v1beta1
kind: HelmRepository
metadata:
  name: charts
  namespace: classy-ns
spec:
  interval: 3m0s
  url: https://one-kubernetes.github.io/classy-helm-charts

---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: dev2-carapuce
  namespace: classy-ns
spec:
  chart:
    spec:
      chart: dev2-carapuce-helm
      sourceRef:
        kind: HelmRepository
        name: charts
        namespace: classy-ns
      version: 0.1.0
  interval: 1m0s
  serviceAccountName: classy
```

----

### C03- Synchro avec le dÃ©pÃ´t Github

AprÃ¨s `git commit && git push`, on obtient cette arborescence.

![classy config files](images/dev2_config_files.png)

---

# Really isolated tenants?

---

## _**ğŸ¸ROCKY**_ is trying to deploy a new color version of its app

- New version with new color
- But introduces a mistake in the deployment.yaml file (wrong NS)
  
commit

See what happensâ€¦

- it might be great to automatically fix this kind of mistake by enforcing the Namespace and the service account that are configured for deploying _**ğŸ¸ROCKY**_ app.

---

## introducing Kyverno

Kyverno is a tool to extend Kubernetes permission management to express complex policies.

---

### Kyverno -- for more info

Reference to the `Kyverno` chapter in High Five M4 module

---

## Download Kyverno distribution
```bash
mkdir -p clusters/mycluster/kyverno
```
```bash
wget https://raw.githubusercontent.com/kyverno/kyverno/v1.5.4/definitions/release/install.yaml -O clusters/mycluster/kyverno/kyverno-components.yaml
```
> :warning: Remember to commit and push your code each time you make a change so that FluxCD can apply the changes.

---

## Install Kyverno on cluster
```bash
flux create kustomization kyverno --prune true --interval 10m --path ./clusters/mycluster/kyverno --wait true --source GitRepository/flux-system --export > ./clusters/mycluster/kyverno/sync.yaml
```
```bash
cd ./clusters/mycluster/kyverno/ && kustomize create --autodetect
```
```bash
cd -
```
> :warning: Remember to commit and push your code each time you make a change so that FluxCD can apply the changes.

---

## Add Kyverno policy to enforce use of Service Account

```bash
mkdir -p clusters/mycluster/kyverno-policies
```
```bash
cat << EOF | tee ./clusters/mycluster/kyverno-policies/enforce-service-account.yaml
---
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: flux-multi-tenancy
spec:
  validationFailureAction: enforce
  rules:
    - name: serviceAccountName
      exclude:
        resources:
          namespaces:
            - flux-system
      match:
        resources:
          kinds:
            - Kustomization
            - HelmRelease
      validate:
        message: ".spec.serviceAccountName is required"
        pattern:
          spec:
            serviceAccountName: "?*"
    - name: kustomizationSourceRefNamespace
      exclude:
        resources:
          namespaces:
            - flux-system
      match:
        resources:
          kinds:
            - Kustomization
      preconditions:
        any:
          - key: "{{request.object.spec.sourceRef.namespace}}"
            operator: NotEquals
            value: ""
      validate:
        message: "spec.sourceRef.namespace must be the same as metadata.namespace"
        deny:
          conditions:
            - key: "{{request.object.spec.sourceRef.namespace}}"
              operator: NotEquals
              value:  "{{request.object.metadata.namespace}}"
    - name: helmReleaseSourceRefNamespace
      exclude:
        resources:
          namespaces:
            - flux-system
      match:
        resources:
          kinds:
            - HelmRelease
      preconditions:
        any:
          - key: "{{request.object.spec.chart.spec.sourceRef.namespace}}"
            operator: NotEquals
            value: ""
      validate:
        message: "spec.chart.spec.sourceRef.namespace must be the same as metadata.namespace"
        deny:
          conditions:
            - key: "{{request.object.spec.chart.spec.sourceRef.namespace}}"
              operator: NotEquals
              value:  "{{request.object.metadata.namespace}}"
EOF
```
```bash
flux create kustomization kyverno-policies --prune true --interval 10m --path ./clusters/mycluster/kyverno-policies --wait true --source GitRepository/flux-system --export > ./clusters/mycluster/kyverno-policies/sync.yaml
```
```bash
cd ./clusters/mycluster/kyverno-policies/ && kustomize create --autodetect
```
```bash
cd -
```
> :warning: Remember to commit and push your code each time you make a change so that FluxCD can apply the changes.

---

## Apply Kyverno policy
```bash
flux create kustomization kyverno-policies --prune true --interval 5m --path ./clusters/mycluster/kyverno-policies --source GitRepository/flux-system --depends-on kyverno --export > ./clusters/mycluster/kyverno-policies/sync.yaml
```
> :warning: Remember to commit and push your code each time you make a change so that FluxCD can apply the changes.

---

## Add Kyverno dependency for _**âš—ï¸TEST**_ cluster
```bash
flux create kustomization tenants --prune true --interval 5m --path ./tenants/test --source GitRepository/flux-system --depends-on kyverno-policies --export > ./clusters/mycluster/tenants.yaml
```
> :warning: Remember to commit and push your code each time you make a change so that FluxCD can apply the changes.

---

## Fix Kyverno policy
```bash
flux create source helm charts --url=https://one-kubernetes.github.io/classy-helm-charts --interval=3m --namespace classy-ns --export > ./tenants/base/classy/sync.yaml
```
```bash
flux create helmrelease dev2-carapuce --namespace=classy-ns --service-account=classy --source=HelmRepository/charts.classy-ns --chart=dev2-carapuce-helm --chart-version="0.1.0" --export >> ./tenants/base/classy/sync.yaml
```
> :warning: Remember to commit and push your code each time you make a change so that FluxCD can apply the changes.

---

# Network leak

ğŸš§ _**ğŸ¸ROCKY**_ upgrades its app and finally create an error because its app is connecting to _**ğŸ»CLASSY**_ database, running in another namespace.
Introducing Pod network policies to avoid such kind of thing.

___

# Switching to _**ğŸšœPROD**_ env

---

## Creating the _**ğŸšœPROD**_ cluster in DigitalÂ Ocean

ğŸš§

---

## Installing and configuring Flux for _**ğŸšœPROD**_ cluster

ğŸš§

Installing Flux
Adding Github source
Adding Kustomize patches so that the cluster is taken into account.

---

# using CloudNativePG instead of stand-alone PostgreSQL on _**ğŸšœPROD**_ cluster

ğŸš§ Instead of having _**ğŸ¸ROCKY**_ team deploying a stand-alone ephemeral PostgreSQL, we'll introduce CloudNativePG and make it available for dev teams to use it as a PostgreSQL provider.


---

# Install cert-manager on _**ğŸšœPROD**_ cluster

ğŸš§ exposing _**ğŸ¸ROCKY**_ team with HTTPs.

---

# Install monitoring stack

---

## Install Prometheus
```bash
mkdir -p clusters/mycluster/kube-prometheus-stack
```
```bash
flux create source helm prometheus-community --url=https://prometheus-community.github.io/helm-charts --interval=1m --export > clusters/mycluster/kube-prometheus-stack/sync.yaml
```
```bash
cat << EOF | tee ./clusters/mycluster/kube-prometheus-stack/values.yaml
alertmanager:
  enabled: false
grafana:
  sidecar:
    dashboards:
      searchNamespace: ALL
prometheus:
  prometheusSpec:
    podMonitorSelector:
      matchLabels:
        app.kubernetes.io/part-of: flux
EOF
```
```bash
flux create helmrelease kube-prometheus-stack --chart kube-prometheus-stack --source HelmRepository/prometheus-community --chart-version 31.0.0 --crds CreateReplace --export --target-namespace monitoring --create-target-namespace true --values ./clusters/mycluster/kube-prometheus-stack/values.yaml >> ./clusters/mycluster/kube-prometheus-stack/sync.yaml
```
> :warning: Remember to commit and push your code each time you make a change so that FluxCD can apply the changes.

---

## Install Flux Grafana dashboards
```bash
mkdir -p clusters/mycluster/kube-prometheus-stack-config
```
```bash
cat << EOF | tee ./clusters/mycluster/kube-prometheus-stack-config/podmonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: flux-system
  namespace: flux-system
  labels:
    app.kubernetes.io/part-of: flux
spec:
  namespaceSelector:
    matchNames:
      - flux-system
  selector:
    matchExpressions:
      - key: app
        operator: In
        values:
          - helm-controller
          - source-controller
          - kustomize-controller
          - notification-controller
          - image-automation-controller
          - image-reflector-controller
  podMetricsEndpoints:
    - port: http-prom
EOF
```
```bash
wget https://raw.githubusercontent.com/fluxcd/flux2/main/manifests/monitoring/grafana/dashboards/cluster.json -P ./clusters/mycluster/kube-prometheus-stack-config/
```
```bash
wget https://raw.githubusercontent.com/fluxcd/flux2/main/manifests/monitoring/grafana/dashboards/control-plane.json -P ./clusters/mycluster/kube-prometheus-stack-config/
```
```bash
cat << EOF | tee ./clusters/mycluster/kube-prometheus-stack-config/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: flux-system
resources:
  - podmonitor.yaml
configMapGenerator:
  - name: flux-grafana-dashboards
    files:
      - control-plane.json
      - cluster.json
    options:
      labels:
        grafana_dashboard: flux-system
EOF
```
> :warning: Remember to commit and push your code each time you make a change so that FluxCD can apply the changes.

---

## Access the Grafana dashboard
```bash
kubectl -n monitoring port-forward svc/monitoring-kube-prometheus-stack-grafana 3000:80
```

## Get the Grafana admin password
```bash
kubectl get secret --namespace kube-prometheus-stack kube-prometheus-stack-grafana -o json | jq '.data | map_values(@base64d)'
```

## And browseâ€¦

---
